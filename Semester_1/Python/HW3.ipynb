{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import nbformat\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotly theme to 'plotly_white'\n",
    "pio.templates.default = 'plotly_white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN.CSV\n",
    "\n",
    "- row_id: (int64) ID code for the row.\n",
    "- timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n",
    "- user_id: (int32) ID code for the user.\n",
    "- content_id: (int16) ID code for the user interaction\n",
    "- content_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n",
    "- task_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n",
    "- user_answer: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n",
    "- answered_correctly: (int8) if the user responded correctly. Read -1 as null, for lectures.\n",
    "- prior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n",
    "- prior_question_had_explanation: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types for each column\n",
    "dtype_train_dict = {\n",
    "    'row_id': 'Int64',\n",
    "    'timestamp': 'Int64',\n",
    "    'user_id': 'Int32',\n",
    "    'content_id': 'Int16',\n",
    "    'content_type_id': 'Int8',\n",
    "    'task_container_id': 'Int16',\n",
    "    'user_answer': 'Int8',\n",
    "    'answered_correctly': 'Int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "}\n",
    "\n",
    "# Read the CSV file with specified dtypes\n",
    "train = pd.read_csv('train.csv', dtype=dtype_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS.CSV: METADATA FOR THE QUESTIONS POSED TO USERS.\n",
    "\n",
    "- question_id: foreign key for the train/test content_id column, when the content type is question (0).\n",
    "- bundle_id: code for which questions are served together.\n",
    "- correct_answer: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n",
    "- part: the relevant section of the TOEIC test.\n",
    "- tags: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types for each column\n",
    "dtype_questions_dict = {\n",
    "    'question_id': 'int16',\n",
    "    'bundle_id': 'int16',\n",
    "    'correct_answer': 'int8',\n",
    "    'part': 'int8'\n",
    "}\n",
    "\n",
    "# Read the CSV file with specified dtypes\n",
    "questions = pd.read_csv('questions.csv', dtype=dtype_questions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LECTURES.CSV: METADATA FOR THE LECTURES WATCHED BY USERS AS THEY PROGRESS IN THEIR EDUCATION.\n",
    "\n",
    "- lecture_id: foreign key for the train/test content_id column, when the content type is lecture (1).\n",
    "- part: top level category code for the lecture.\n",
    "- tag: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n",
    "- type_of: brief description of the core purpose of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types for each column\n",
    "dtype_questions_dict = {\n",
    "    'lecture_id': 'int16',\n",
    "    'tag': 'int16',\n",
    "    'part': 'int8'\n",
    "}\n",
    "\n",
    "# Read the CSV file with specified dtypes\n",
    "lectures = pd.read_csv('lectures.csv', dtype=dtype_questions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = train.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"row_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on all columns\n",
    "duplicates = train.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows before dropping duplicates\n",
    "num_rows_before = train.shape[0]\n",
    "print(f\"Number of rows before dropping duplicates: {num_rows_before}\")\n",
    "\n",
    "# Drop duplicates (keeping the first occurrence by default)\n",
    "train.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# Count the number of rows after dropping duplicates\n",
    "num_rows_after = train.shape[0]\n",
    "print(f\"Number of rows after dropping duplicates: {num_rows_after}\")\n",
    "\n",
    "# Print the difference\n",
    "num_duplicates_dropped = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicates dropped: {num_duplicates_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'prior_question_elapsed_time' to seconds\n",
    "train['prior_question_elapsed_time'] = train['prior_question_elapsed_time'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby 'user_answer' and count the occurrences\n",
    "user_answer_counts = train['user_answer'].value_counts().reset_index()\n",
    "user_answer_counts.columns = ['user_answer', 'count']\n",
    "\n",
    "# Plot the counts of 'user_answer' using Plotly\n",
    "fig = px.bar(user_answer_counts, x='user_answer', y='count', \n",
    "             title='Distribution of User Answers',\n",
    "             labels={'user_answer': 'User Answer', 'count': 'Count'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_answer_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby 'answered_correctly' and count the occurrences\n",
    "answered_correctly_counts = train['answered_correctly'].value_counts().reset_index()\n",
    "answered_correctly_counts.columns = ['answered_correctly', 'count']\n",
    "\n",
    "# Plot the counts of 'answered_correctly' using Plotly\n",
    "fig = px.bar(answered_correctly_counts, x='answered_correctly', y='count',\n",
    "             title='Distribution of Answers (Correct/Incorrect)',\n",
    "             labels={'answered_correctly': 'Answered Correctly', 'count': 'Count'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answered_correctly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check percentage of unknown 'answered_correctly'\n",
    "percentage = (train['answered_correctly'] == -1).mean() * 100\n",
    "print(f\"Percentage of rows with unknown answered: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate content into questions and lectures\n",
    "questions_train_df = train[train['content_type_id'] == 0]\n",
    "lectures_train_df = train[train['content_type_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_df = pd.merge(\n",
    "    left=questions_train_df, right=questions, \n",
    "    left_on=\"content_id\", right_on=\"question_id\",\n",
    ")\n",
    "\n",
    "questions_train_df = questions_train_df.drop([\"question_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_train_df = pd.merge(\n",
    "    left=lectures_train_df, right=lectures, \n",
    "    left_on=\"content_id\", right_on=\"lecture_id\",\n",
    ")\n",
    "\n",
    "lectures_train_df = lectures_train_df.drop([\"lecture_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 1: Impact of Prior Explanation on Correct Answer Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_group = questions_train_df.groupby('prior_question_had_explanation')['answered_correctly'].mean().reset_index()\n",
    "\n",
    "fig1 = px.bar(\n",
    "    explanation_group,\n",
    "    x='prior_question_had_explanation',\n",
    "    y='answered_correctly',\n",
    "    title='Impact of Prior Explanation on Correct Answer Rate',\n",
    "    labels={'prior_question_had_explanation': 'Prior Question Had Explanation', 'answered_correctly': 'Average Correct Answer Rate'},\n",
    "    text='answered_correctly'\n",
    ")\n",
    "fig1.update_traces(texttemplate='%{text:.2f}', textposition='outside')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del explanation_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_correct = questions_train_df.groupby(['prior_question_had_explanation', 'answered_correctly']).size().reset_index(name='count')\n",
    "fig = px.bar(explanation_correct, x='prior_question_had_explanation', y='count', color='answered_correctly',\n",
    "             title='Impact of Prior Question Explanation on Answer Correctness',\n",
    "             labels={'prior_question_had_explanation': 'Prior Question Had Explanation', 'count': 'Number of Responses', 'answered_correctly': 'Answered Correctly'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 2: Average Time Taken vs Correct Answer Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_correct_group = questions_train_df.groupby(pd.cut(questions_train_df['prior_question_elapsed_time'], bins=10))['answered_correctly'].mean().reset_index()\n",
    "time_correct_group['prior_question_elapsed_time'] = time_correct_group['prior_question_elapsed_time'].astype(str)\n",
    "\n",
    "fig2 = px.line(\n",
    "    time_correct_group,\n",
    "    x='prior_question_elapsed_time',\n",
    "    y='answered_correctly',\n",
    "    title='Average Time Taken vs Correct Answer Rate',\n",
    "    labels={'prior_question_elapsed_time': 'Prior Question Elapsed Time (binned)', 'answered_correctly': 'Average Correct Answer Rate'}\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 3: Correct Answer Rate by Lecture Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_lecture_count = lectures_train_df['part'].value_counts().reset_index()\n",
    "part_lecture_count.columns = ['part', 'lecture_count']\n",
    "\n",
    "fig3 = px.bar(\n",
    "    part_lecture_count,\n",
    "    x='part',\n",
    "    y='lecture_count',\n",
    "    title='Lecture Count by Part',\n",
    "    labels={'part': 'Part', 'lecture_count': 'Lecture Count'},\n",
    "    text='lecture_count'\n",
    ")\n",
    "fig3.update_traces(texttemplate='%{text:.2f}', textposition='outside')\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 4: Distribution of User Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_answer_dist = questions_train_df['user_answer'].value_counts().reset_index()\n",
    "user_answer_dist.columns = ['user_answer', 'count']\n",
    "\n",
    "fig4 = px.bar(\n",
    "    user_answer_dist,\n",
    "    x='user_answer',\n",
    "    y='count',\n",
    "    title='Distribution of User Answers',\n",
    "    labels={'user_answer': 'User Answer', 'count': 'Count'},\n",
    "    text='count'\n",
    ")\n",
    "fig4.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 5: Average Correct Answer Rate by Task Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_container_group = questions_train_df[questions_train_df['answered_correctly'] != -1].groupby('task_container_id')['answered_correctly'].mean().reset_index()\n",
    "\n",
    "fig5 = px.line(\n",
    "    task_container_group,\n",
    "    x='task_container_id',\n",
    "    y='answered_correctly',\n",
    "    title='Average Correct Answer Rate by Task Container',\n",
    "    labels={'task_container_id': 'Task Container ID', 'answered_correctly': 'Average Correct Answer Rate'}\n",
    ")\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 6: Lecture Types Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_type_dist = lectures_train_df['type_of'].value_counts().reset_index()\n",
    "lecture_type_dist.columns = ['type_of', 'count']\n",
    "\n",
    "fig6 = px.pie(\n",
    "    lecture_type_dist,\n",
    "    names='type_of',\n",
    "    values='count',\n",
    "    title='Lecture Types Distribution'\n",
    ")\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 7: Lecture Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_type_correct = lectures_train_df.groupby('type_of').agg(\n",
    "    lecture_count=('type_of', 'size'),\n",
    "    avg_correct_answer_rate=('answered_correctly', 'mean')\n",
    ").reset_index()\n",
    "lecture_type_correct.dropna(subset=['avg_correct_answer_rate'], inplace=True)\n",
    "\n",
    "fig7 = px.bar(\n",
    "    lecture_type_correct,\n",
    "    x='type_of',\n",
    "    y='lecture_count',\n",
    "    title='Lecture Count by Type',\n",
    "    labels={'type_of': 'Lecture Type', 'lecture_count': 'Lecture Count'},\n",
    "    text='lecture_count'\n",
    ")\n",
    "fig7.update_traces(texttemplate='%{text:.2f}', textposition='outside')\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 8: Correct Answer Rate by User ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correct_group = train[train['answered_correctly'] != -1].groupby('user_id')['answered_correctly'].mean().reset_index()\n",
    "fig = px.histogram(user_correct_group, x='answered_correctly', nbins=30,\n",
    "                   title='Distribution of Average Correct Answer Rate by User',\n",
    "                   labels={'answered_correctly': 'Average Correct Answer Rate', 'count': 'Number of Users'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 8: Average Elapsed Time by Correct/Incorrect Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(questions_train_df, x='answered_correctly', y='prior_question_elapsed_time',\n",
    "             title='Prior Question Elapsed Time by Answer Correctness',\n",
    "             labels={'answered_correctly': 'Answered Correctly', 'prior_question_elapsed_time': 'Elapsed Time (milliseconds)'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([questions_train_df, lectures_train_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "del questions_train_df\n",
    "del lectures_train_df\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "\n",
    "# # Assuming `questions_train_merged` and `lectures_train_df` are already loaded DataFrames\n",
    "\n",
    "# # Get unique user_ids from both datasets\n",
    "# user_ids_lectures = set(lectures_train_df['user_id'].unique())\n",
    "# user_ids_questions = set(questions_train_df['user_id'].unique())\n",
    "\n",
    "# # Check which users attended lectures and answered questions\n",
    "# users_in_lectures = user_ids_questions.intersection(user_ids_lectures)\n",
    "# users_not_in_lectures = user_ids_questions - user_ids_lectures\n",
    "\n",
    "# # Filter records of users who attended lectures and those who did not\n",
    "# users_in_lectures_df = questions_train_df[questions_train_df['user_id'].isin(users_in_lectures)]\n",
    "# users_not_in_lectures_df = questions_train_df[questions_train_df['user_id'].isin(users_not_in_lectures)]\n",
    "\n",
    "# # Calculate the average correct answer rate for users who attended lectures and those who did not\n",
    "# user_correct_answer_rate_in_lectures = users_in_lectures_df.groupby('user_id')['answered_correctly'].mean().reset_index()\n",
    "# user_correct_answer_rate_in_lectures['group'] = 'Attended Lectures'\n",
    "\n",
    "# user_correct_answer_rate_not_in_lectures = users_not_in_lectures_df.groupby('user_id')['answered_correctly'].mean().reset_index()\n",
    "# user_correct_answer_rate_not_in_lectures['group'] = 'Did Not Attend Lectures'\n",
    "\n",
    "# # Concatenate the two groups\n",
    "# user_correct_answer_rate = pd.concat([user_correct_answer_rate_in_lectures, user_correct_answer_rate_not_in_lectures], ignore_index=True)\n",
    "\n",
    "# # Plot the average correct answer rate for users attending vs not attending lectures using Plotly\n",
    "# fig = px.histogram(user_correct_answer_rate, x='answered_correctly', color='group', barmode='overlay', nbins=30,\n",
    "#                    title='Distribution of Average Correct Answer Rate: Attended vs Did Not Attend Lectures',\n",
    "#                    labels={'answered_correctly': 'Average Correct Answer Rate', 'count': 'Number of Users', 'group': 'User Group'},\n",
    "#                    color_discrete_sequence=['skyblue', 'salmon'])\n",
    "# fig.update_layout(xaxis_title='Average Correct Answer Rate', yaxis_title='Number of Users')\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
