{"cells":[{"cell_type":"markdown","metadata":{"id":"A79WfSKcSE1s"},"source":["# Содержание\n","- [PCA](#1)\n","- [t-SNE](#2)"]},{"cell_type":"markdown","metadata":{"id":"5HTjs72DUJiB"},"source":["<a name='1'></a>\n","## PCA\n","\n","PCA(Principal Component Analysis) - это метод снижения размерности данных, который используется для выявления наиболее информативных признаков в наборе данных и проекции их на новое пространство меньшей размерности. Основная цель PCA - найти линейные комбинации исходных признаков, называемые главными компонентами, которые содержат наибольшую дисперсию в данных.\n","\n","Процесс работы PCA следующий:\n","\n","1. Стандартизация данных: Исходные признаки стандартизируются, чтобы они имели среднее значение равное 0 и стандартное отклонение равное 1. Это делается для того, чтобы признаки с различными единицами измерения не искажали результаты анализа.\n","2. Вычисление матрицы ковариации: Вычисляется матрица ковариации, которая показывает связи исходных признаков друг с другом. Ковариация измеряет, насколько два признака меняются вместе.\n","3. Вычисление собственных значений и собственных векторов: Собственные значения и собственные векторы извлекаются из матрицы ковариации. Собственные значения представляют собой меру дисперсии вдоль соответствующих собственных векторов.\n","4. Сортировка главных компонент: Главные компоненты сортируются по убыванию их собственных значений. Главная компонента с наибольшим собственным значением содержит наибольшую дисперсию в данных.\n","5. Выбор количества главных компонент: Определяется количество главных компонент, которые будут использоваться для проекции данных на новое пространство. Можно выбрать определенное количество компонент или определить процент дисперсии, которую они объясняют.\n","6. Проекция данных: Исходные признаки проецируются на выбранные главные компоненты, формируя новое пространство с меньшей размерностью.\n","\n","Результат PCA - это новое пространство признаков с меньшей размерностью, где каждая главная компонента представляет собой линейную комбинацию исходных признаков. Главные компоненты упорядочены по убыванию их значимости, и первые компоненты содержат наибольшую долю дисперсии в данных. PCA широко используется для визуализ\n","\n","ации данных, устранения мультиколлинеарности, сжатия данных и улучшения производительности алгоритмов машинного обучения путем снижения размерности пространства признаков."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FfS3k7-UJiB"},"outputs":[],"source":["import numpy as np\n","\n","class PCA:\n","    def __init__(self, n_components):\n","        self.n_components = n_components\n","        self.components = None\n","        self.mean = None\n","\n","    def fit(self, X):\n","        # Вычисляем среднее значение каждого признака\n","        self.mean = np.mean(X, axis=0)\n","\n","        # Центрируем данные путем вычитания среднего\n","        X_centered = X - self.mean\n","\n","        # Вычисляем матрицу ковариации\n","        covariance_matrix = np.cov(X_centered.T)\n","\n","        # Вычисляем собственные значения и собственные векторы\n","        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n","\n","        # Сортируем собственные значения и соответствующие собственные векторы в порядке убывания\n","        eigen_indices = np.argsort(eigenvalues)[::-1]\n","        sorted_eigenvectors = eigenvectors[:, eigen_indices]\n","\n","        # Выбираем первые n_components собственных векторов\n","        self.components = sorted_eigenvectors[:, :self.n_components]\n","\n","    def transform(self, X):\n","        # Центрируем данные путем вычитания среднего\n","        X_centered = X - self.mean\n","\n","        # Проецируем данные на главные компоненты\n","        transformed = np.dot(X_centered, self.components)\n","\n","        return transformed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gp0usk_BUJiB"},"outputs":[],"source":["# Пример использования\n","X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n","n_components = 2\n","\n","pca = PCA(n_components)\n","pca.fit(X)\n","\n","transformed = pca.transform(X)\n","print(\"Преобразованные данные:\")\n","print(transformed)"]},{"cell_type":"markdown","metadata":{"id":"csYF0qB4tRXR"},"source":["Рассмотрим пример на реальных данных и визуализацию, чтобы увидеть, как библиотечный PCA работает в реальной задаче. Используем набор данных Iris, который часто применяется для демонстрации анализа данных. PCA поможет нам визуализировать многомерные данные в двухмерном пространстве."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCKZhlQG-HeG"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fs41PPVtGlb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","\n","# Загружаем данные Iris\n","data = load_iris()\n","X_iris = data.data  # признаки\n","y_iris = data.target  # метки классов\n","target_names = data.target_names\n","\n","# Стандартизация данных\n","scaler = StandardScaler()\n","X_iris_scaled = scaler.fit_transform(X_iris)\n","\n","# Применяем PCA для снижения размерности до 2D\n","pca_iris = PCA(n_components=2)\n","X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n","\n","# Визуализируем результаты\n","plt.figure(figsize=(8, 6))\n","colors = ['coral', 'darkkhaki', 'teal']\n","\n","for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n","    plt.scatter(X_iris_pca[y_iris == i, 0], X_iris_pca[y_iris == i, 1], color=color, alpha=0.8, label=target_name)\n","\n","plt.legend(loc='best', shadow=False, scatterpoints=1)\n","plt.title(\"PCA на данных Iris\")\n","plt.xlabel(\"Первая главная компонента\")\n","plt.ylabel(\"Вторая главная компонента\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Ay_zDgOgtVxR"},"source":["Вопросы для обсуждения:\n","\n","- Что вы замечаете в распределении классов?\n","- Есть ли классы, которые пересекаются?\n","- Какое преимущество даёт снижение размерности для визуализации и анализа?\n","- Могут ли потери информации при снижении размерности повлиять на результаты анализа?"]},{"cell_type":"markdown","metadata":{"id":"lc-sNmJk-tj3"},"source":["Посмотрим, как меняется точность обученной модели при использовании данных пониженной размерности. Ожидание: поскольку данные хорошо разделяются визуально, скорее всего потери в качестве не будет."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UL-J69yV-sqv"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Разделяем данные на обучающую и тестовую выборки\n","X_train, X_test, y_train, y_test = train_test_split(X_iris_scaled, y_iris, test_size=0.3, random_state=42)\n","\n","# Модель без PCA\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","accuracy_no_pca = accuracy_score(y_test, y_pred)\n","\n","# Модель с PCA\n","X_train_pca = pca_iris.transform(X_train)\n","X_test_pca = pca_iris.transform(X_test)\n","clf.fit(X_train_pca, y_train)\n","y_pred_pca = clf.predict(X_test_pca)\n","accuracy_with_pca = accuracy_score(y_test, y_pred_pca)\n","\n","print(f\"Точность без PCA: {accuracy_no_pca:.2f}\")\n","print(f\"Точность с PCA: {accuracy_with_pca:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"gt47GWgwFwEJ"},"source":["Рассмотрим более сложные данные: набор изображений с цифрами от 0 до 9."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uApDwJgwl7O"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","\n","# Загружаем данные\n","digits = load_digits()\n","X_digits = digits.data\n","y_digits = digits.target\n","\n","# Применяем PCA\n","pca_digits = PCA(n_components=2)\n","X_digits_pca = pca_digits.fit_transform(X_digits)\n","\n","# Визуализация\n","plt.figure(figsize=(10, 8))\n","for i in range(10):\n","    plt.scatter(X_digits_pca[y_digits == i, 0], X_digits_pca[y_digits == i, 1], label=f'Цифра {i}', alpha=0.6)\n","plt.legend()\n","plt.title(\"PCA для набора данных Digits\")\n","plt.xlabel(\"Первая главная компонента\")\n","plt.ylabel(\"Вторая главная компонента\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"F0GVtfE_GKC4"},"source":["Разделение между классами гораздо менее очевидное. Можно ожидать снижение точности при обучении модели на данных пониженной размерности."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOFZhiRaweQg"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Разделяем данные на обучающую и тестовую выборки\n","X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, test_size=0.4, random_state=42)\n","\n","# Модель без PCA\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","accuracy_no_pca = accuracy_score(y_test, y_pred)\n","\n","# Модель с PCA\n","X_train_pca = pca_digits.transform(X_train)\n","X_test_pca = pca_digits.transform(X_test)\n","clf.fit(X_train_pca, y_train)\n","y_pred_pca = clf.predict(X_test_pca)\n","accuracy_with_pca = accuracy_score(y_test, y_pred_pca)\n","\n","print(f\"Точность без PCA: {accuracy_no_pca:.2f}\")\n","print(f\"Точность с PCA: {accuracy_with_pca:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"0JNdOxyPGTJh"},"source":["Падение точности после применения PCA — вполне ожидаемое явление, особенно если происходит значительная потеря информации при снижении размерности.\n","Но это снижение в точности происходит за счёт выигрыша во времени обучения. Посмотрим, как меняются точность и время затраченное на обучение в зависимости от числа компонент."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zR9yexEZ_DsR"},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGu41XrtDHbe"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Данные для примера (используем результаты из цикла с компонентами)\n","# components_range = range(1, 51, 5)  # Число компонент PCA\n","components_range = range(1, X_train.shape[1] + 1)\n","training_times = []  # Список для времени обучения\n","accuracies = []  # Список для точности модели\n","\n","for n in components_range:\n","    pca = PCA(n_components=n)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","\n","    start_time = time.time()\n","    clf = RandomForestClassifier(random_state=42)\n","    clf.fit(X_train_pca, y_train)\n","    y_pred = clf.predict(X_test_pca)\n","    training_times.append(time.time() - start_time)\n","    accuracies.append(accuracy_score(y_test, y_pred))\n","\n","# Построение графика\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","# Левая ось: точность\n","ax1.set_xlabel('Число компонент PCA')\n","ax1.set_ylabel('Точность', color='blue')\n","ax1.plot(components_range, accuracies, marker='o', color='blue', label='Точность')\n","ax1.tick_params(axis='y', labelcolor='blue')\n","ax1.grid(visible=True, linestyle='--', alpha=0.5)\n","\n","# Правая ось: время обучения\n","ax2 = ax1.twinx()\n","ax2.set_ylabel('Время обучения (с)', color='orange')\n","ax2.plot(components_range, training_times, marker='s', color='orange', label='Время обучения')\n","ax2.tick_params(axis='y', labelcolor='orange')\n","\n","# Заголовок и легенда\n","fig.suptitle('Влияние числа компонент PCA на точность и время обучения', fontsize=14)\n","fig.tight_layout()\n","\n","# Отображение графика\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"yhkpLbqYJdHf"},"source":["В данном случае, точность в обучении между полноразмерными данными и данными с размерностью, уменьшенной до 15-20 компонент практически не меняется, при этом время обучения на данных меньшей размерности оказыватеся в полтора-два раза короче."]},{"cell_type":"markdown","metadata":{"id":"XRyqSDVWUJiB"},"source":["<a name='2'></a>\n","## t-SNE\n","\n","t-SNE (t-Distributed Stochastic Neighbor Embedding) - это метод снижения размерности данных, который используется для визуализации сложных структур данных и обнаружения скрытых паттернов. В отличие от PCA, t-SNE обеспечивает сохранение не только линейной структуры данных, но и нелинейных отношений между точками.\n","\n","Процесс работы t-SNE следующий:\n","\n","1. Вычисление аффинности: Сначала вычисляется аффинность (похожесть) между парами точек данных. Это может быть сделано с использованием Гауссовой функции, основанной на расстоянии между точками в исходном пространстве.\n","2. Вычисление условной вероятности: Для каждой точки данных вычисляется условная вероятность, которая показывает вероятность выбрать другую точку в качестве соседа, исходя из аффинности. Более похожие точки имеют более высокие вероятности быть выбранными в качестве соседей.\n","3. Определение сходства в пространстве низкой размерности: Для пространства низкой размерности (обычно 2D) исходные точки данных и их аффинности переопределяются с использованием условных вероятностей. Это позволяет сохранить близость точек, имеющих высокую аффинность в исходном пространстве.\n","4. Минимизация дивергенции Кульбака-Лейблера: t-SNE оптимизирует распределение точек в пространстве низкой размерности, минимизируя дивергенцию Кульбака-Лейблера между условными вероятностями точек в исходном пространстве и пространстве низкой размерности.\n","5. Итерационная оптимизация: Алгоритм итеративно обновляет расположение точек в пространстве низкой размерности, чтобы минимизировать дивергенцию Кульбака-Лейблера. Оптимизация основывается на градиентных методах или методах случайного блуждания.\n","\n","Результат t-SNE представляет собой вложение точек в пространство низкой размерности, где близкие точки соответствуют точкам с высокой аффинностью в исходном пространстве. t-SNE обладает способностью выявлять сложные нелинейные структуры и кластеры в данных, что делает его полезным для визуализации и понимания сложных наборов данных. Однако важно отметить, что расположение точек в пространстве низкой размерности может быть чувствительным к различным параметрам и инициализации, поэтому результаты t-SNE требуют внимательного анализа и интерпретации."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Naxcge2UwVF"},"outputs":[],"source":["import numpy as np\n","from sklearn.decomposition import PCA\n","\n","class TSNE:\n","    def __init__(self, n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=None):\n","        \"\"\"\n","        Инициализация параметров t-SNE.\n","        n_components: размерность целевого пространства (обычно 2 или 3).\n","        perplexity: целевая сложность (число \"эффективных\" соседей для каждого объекта).\n","        learning_rate: скорость обучения (шаг градиентного спуска).\n","        n_iter: количество итераций оптимизации.\n","        random_state: начальное значение для генератора случайных чисел.\n","        \"\"\"\n","        self.n_components = n_components\n","        self.perplexity = perplexity\n","        self.learning_rate = learning_rate\n","        self.n_iter = n_iter\n","        self.random_state = random_state\n","\n","    def _compute_pairwise_affinities(self, X):\n","        \"\"\"\n","        Расчет матрицы вероятностей P в высокоразмерном пространстве на основе perplexity.\n","        Используется бинарный поиск для подбора параметра sigma (β = 1 / sigma^2).\n","        \"\"\"\n","        n = X.shape[0]  # Количество точек\n","        distances = np.square(X[:, np.newaxis] - X).sum(axis=2)  # Квадрат Евклидовых расстояний\n","        sigmas = np.ones(n)  # Инициализация сигм\n","        P = np.zeros((n, n))  # Матрица вероятностей P\n","\n","        for i in range(n):\n","            # Бинарный поиск для настройки sigma\n","            beta_min, beta_max = None, None\n","            beta = 1.0\n","            for _ in range(50):  # Максимум 50 итераций поиска\n","                P_i = np.exp(-distances[i] * beta)  # Вычисление вероятностей\n","                P_i[i] = 0  # Зануляем вероятность для самого себя\n","                sum_P_i = P_i.sum()  # Сумма вероятностей\n","                H_i = np.log(sum_P_i) + beta * (distances[i] * P_i).sum() / sum_P_i  # Энтропия\n","                P[i] = P_i / sum_P_i  # Нормализация\n","\n","                # Проверка разницы между текущей и целевой энтропией\n","                H_diff = H_i - np.log(self.perplexity)\n","                if np.abs(H_diff) < 1e-5:  # Если достигнута точность\n","                    break\n","                if H_diff > 0:  # Если текущая энтропия больше целевой\n","                    beta_min = beta\n","                    beta = (beta + beta_max) / 2 if beta_max else beta * 2\n","                else:  # Если текущая энтропия меньше целевой\n","                    beta_max = beta\n","                    beta = (beta + beta_min) / 2 if beta_min else beta / 2\n","\n","        # Симметризация P и нормализация\n","        P = (P + P.T) / (2 * n)\n","        return P\n","\n","    def _compute_joint_probabilities_q(self, Y):\n","        \"\"\"\n","        Расчет вероятностей Q в низкоразмерном пространстве.\n","        \"\"\"\n","        distances = np.square(Y[:, np.newaxis] - Y).sum(axis=2)  # Квадрат расстояний в низкоразмерном пространстве\n","        Q = 1 / (1 + distances)  # Вычисление Q по формуле t-распределения\n","        np.fill_diagonal(Q, 0)  # Обнуляем диагональные элементы\n","        Q /= Q.sum()  # Нормализация Q\n","        return Q\n","\n","    def _compute_gradient(self, P, Q, Y):\n","        \"\"\"\n","        Вычисление градиента Kullback-Leibler дивергенции.\n","        \"\"\"\n","        pq_diff = P - Q  # Разница между высоко- и низкоразмерными вероятностями\n","        distances = 1 + np.square(Y[:, np.newaxis] - Y).sum(axis=2)  # 1 + ||y_i - y_j||^2\n","        weights = pq_diff[:, :, np.newaxis] / distances[:, :, np.newaxis]  # Вес градиента\n","        grad = 4 * (weights * (Y[:, np.newaxis] - Y)).sum(axis=1)  # Итоговый градиент\n","        return grad\n","\n","    def fit_transform(self, X):\n","        \"\"\"\n","        Основной метод: выполнение алгоритма t-SNE.\n","        Возвращает преобразованные точки в низкоразмерном пространстве.\n","        \"\"\"\n","        np.random.seed(self.random_state)  # Фиксация генератора случайных чисел\n","\n","        # Шаг 1: расчет матрицы P\n","        P = self._compute_pairwise_affinities(X)\n","        P *= 4  # Раннее увеличение (early exaggeration)\n","\n","        # Шаг 2: Инициализация точек в низкоразмерном пространстве\n","        Y = np.random.normal(0, 1e-4, (X.shape[0], self.n_components))\n","\n","        # Итеративная оптимизация\n","        for iter in range(self.n_iter):\n","            Q = self._compute_joint_probabilities_q(Y)  # Расчет Q\n","            grad = self._compute_gradient(P, Q, Y)  # Расчет градиента\n","\n","            # Градиентный спуск\n","            Y -= self.learning_rate * grad\n","\n","            # Уменьшение раннего увеличения после 250 итераций\n","            if iter == 250:\n","                P /= 4\n","\n","            # Логирование на каждой сотой итерации\n","            if iter % 100 == 0 or iter == self.n_iter - 1:\n","                kl_div = np.sum(P * np.log((P + 1e-12) / (Q + 1e-12)))  # Расчет KL-дивергенции\n","                print(f\"Итерация {iter}, KL-дивергенция: {kl_div:.5f}\")\n","\n","        return Y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54lh31WlTuNf"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris, load_digits\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import TSNE as SKLEARN_TSNE  # Для сравнения с вашим методом\n","\n","\n","# Функция для визуализации\n","def plot_tsne(embedding, labels, title):\n","    plt.figure(figsize=(8, 6))\n","    scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='viridis', alpha=0.7)\n","    plt.colorbar(scatter, label=\"Классы\")\n","    plt.title(title, fontsize=16)\n","    plt.xlabel(\"Первая компонента\", fontsize=12)\n","    plt.ylabel(\"Вторая компонента\", fontsize=12)\n","    plt.show()\n","\n","# Данные Iris\n","iris = load_iris()\n","X_iris = iris.data\n","y_iris = iris.target\n","\n","# Данные Digits\n","digits = load_digits()\n","X_digits = digits.data\n","y_digits = digits.target\n","\n","# Стандартизация данных\n","scaler = StandardScaler()\n","X_iris_scaled = scaler.fit_transform(X_iris)\n","X_digits_scaled = scaler.fit_transform(X_digits)\n","\n","# Применение кастомного TSNE для Iris\n","tsne_custom_iris = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=500)\n","X_embedded_iris = tsne_custom_iris.fit_transform(X_iris_scaled)\n","plot_tsne(X_embedded_iris, y_iris, \"t-SNE на данных Iris (кастомный)\")\n","\n","# Сравнение с sklearn t-SNE\n","sklearn_tsne_iris = SKLEARN_TSNE(n_components=2, perplexity=30, learning_rate=200, max_iter=500, random_state=0)\n","embedding_sklearn_iris = sklearn_tsne_iris.fit_transform(X_iris_scaled)\n","plot_tsne(embedding_sklearn_iris, y_iris, \"t-SNE на данных Iris (sklearn)\")\n","\n","# Применение кастомного TSNE для Digits\n","tsne_custom_digits = TSNE(n_components=2, perplexity=5, learning_rate=200, n_iter=250)\n","X_embedded_digits = tsne_custom_digits.fit_transform(X_digits_scaled)\n","plot_tsne(X_embedded_digits, y_digits, \"t-SNE на данных Digits (кастомный)\")\n","\n","# Сравнение с sklearn t-SNE\n","sklearn_tsne_digits = SKLEARN_TSNE(n_components=2, perplexity=5, learning_rate=200, max_iter=250, random_state=0)\n","embedding_sklearn_digits = sklearn_tsne_digits.fit_transform(X_digits_scaled)\n","plot_tsne(embedding_sklearn_digits, y_digits, \"t-SNE на данных Iris (sklearn)\")"]},{"cell_type":"markdown","metadata":{"id":"8TFPxnQRLXPp"},"source":["Теперь сравним эффективность работы библиотечных имплементаций t-SNE и PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEC-E0ycHO44"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.datasets import load_iris, load_digits\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Настройки для визуализаций\n","sns.set(style=\"whitegrid\", font_scale=1.2, palette=\"Set2\")\n","\n","# Применение t-SNE\n","tsne_iris = TSNE(n_components=2, perplexity=30, random_state=42)\n","X_iris_tsne = tsne_iris.fit_transform(X_iris_scaled)\n","\n","# Сравнение PCA и t-SNE\n","pca_iris = PCA(n_components=2)\n","X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# PCA\n","sns.scatterplot(\n","    x=X_iris_pca[:, 0], y=X_iris_pca[:, 1], hue=y_iris, palette=\"viridis\", ax=ax[0], s=50, alpha=0.7\n",")\n","ax[0].set_title(\"PCA Visualization of Iris Dataset\", fontsize=14)\n","ax[0].set_xlabel(\"Principal Component 1\")\n","ax[0].set_ylabel(\"Principal Component 2\")\n","# ax[0].legend(title=\"Classes\", labels=iris.target_names)\n","\n","# t-SNE\n","sns.scatterplot(\n","    x=X_iris_tsne[:, 0], y=X_iris_tsne[:, 1], hue=y_iris, palette=\"viridis\", ax=ax[1], s=50, alpha=0.7\n",")\n","ax[1].set_title(\"t-SNE Visualization of Iris Dataset\", fontsize=14)\n","ax[1].set_xlabel(\"t-SNE Component 1\")\n","ax[1].set_ylabel(\"t-SNE Component 2\")\n","# ax[1].legend(title=\"Classes\", labels=iris.target_names)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Работа с digits dataset\n","\n","# Применение t-SNE\n","tsne_digits = TSNE(n_components=2, perplexity=30, random_state=42)\n","X_digits_tsne = tsne_digits.fit_transform(X_digits_scaled)\n","\n","# Сравнение PCA и t-SNE\n","pca_digits = PCA(n_components=2)\n","X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# Визуализация t-SNE результатов\n","sns.scatterplot(\n","    x=X_digits_pca[:, 0], y=X_digits_pca[:, 1], hue=y_digits, palette=\"tab10\", ax=ax[0], s=50, alpha=0.7\n",")\n","ax[0].set_title(\"PCA Visualization of Digits Dataset\", fontsize=16)\n","ax[0].set_xlabel(\"PCA Component 1\")\n","ax[0].set_ylabel(\"PCA Component 2\")\n","# plt.legend(title=\"Digits\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n","\n","\n","# Визуализация t-SNE результатов\n","sns.scatterplot(\n","    x=X_digits_tsne[:, 0], y=X_digits_tsne[:, 1], hue=y_digits, palette=\"tab10\", ax=ax[1], s=50, alpha=0.7\n",")\n","ax[1].set_title(\"t-SNE Visualization of Digits Dataset\", fontsize=16)\n","ax[1].set_xlabel(\"t-SNE Component 1\")\n","ax[1].set_ylabel(\"t-SNE Component 2\")\n","# plt.legend(title=\"Digits\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYhyeRGNqEtO"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.datasets import load_iris, load_digits\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import time\n","\n","# Настройки для визуализаций\n","sns.set(style=\"whitegrid\", font_scale=1.2, palette=\"Set2\")\n","\n","# --- Работа с Iris dataset ---\n","# Измерение времени для t-SNE\n","start_tsne = time.time()\n","tsne_iris = TSNE(n_components=2, perplexity=30, random_state=42)\n","X_iris_tsne = tsne_iris.fit_transform(X_iris_scaled)\n","time_tsne_iris = time.time() - start_tsne\n","\n","# Измерение времени для PCA\n","start_pca = time.time()\n","pca_iris = PCA(n_components=2)\n","X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n","time_pca_iris = time.time() - start_pca\n","\n","# Визуализация результатов для Iris dataset\n","fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# PCA\n","sns.scatterplot(\n","    x=X_iris_pca[:, 0], y=X_iris_pca[:, 1], hue=y_iris, palette=\"viridis\", ax=ax[0], s=50, alpha=0.7\n",")\n","ax[0].set_title(f\"PCA Visualization of Iris Dataset\\n(Time: {time_pca_iris:.2f} seconds)\", fontsize=14)\n","ax[0].set_xlabel(\"Principal Component 1\")\n","ax[0].set_ylabel(\"Principal Component 2\")\n","\n","# t-SNE\n","sns.scatterplot(\n","    x=X_iris_tsne[:, 0], y=X_iris_tsne[:, 1], hue=y_iris, palette=\"viridis\", ax=ax[1], s=50, alpha=0.7\n",")\n","ax[1].set_title(f\"t-SNE Visualization of Iris Dataset\\n(Time: {time_tsne_iris:.2f} seconds)\", fontsize=14)\n","ax[1].set_xlabel(\"t-SNE Component 1\")\n","ax[1].set_ylabel(\"t-SNE Component 2\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# --- Работа с Digits dataset ---\n","# Измерение времени для t-SNE\n","start_tsne = time.time()\n","tsne_digits = TSNE(n_components=2, perplexity=30, random_state=42)\n","X_digits_tsne = tsne_digits.fit_transform(X_digits_scaled)\n","time_tsne_digits = time.time() - start_tsne\n","\n","# Измерение времени для PCA\n","start_pca = time.time()\n","pca_digits = PCA(n_components=2)\n","X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n","time_pca_digits = time.time() - start_pca\n","\n","# Визуализация результатов для Digits dataset\n","fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# PCA\n","sns.scatterplot(\n","    x=X_digits_pca[:, 0], y=X_digits_pca[:, 1], hue=y_digits, palette=\"tab10\", ax=ax[0], s=50, alpha=0.7\n",")\n","ax[0].set_title(f\"PCA Visualization of Digits Dataset\\n(Time: {time_pca_digits:.2f} seconds)\", fontsize=16)\n","ax[0].set_xlabel(\"PCA Component 1\")\n","ax[0].set_ylabel(\"PCA Component 2\")\n","\n","# t-SNE\n","sns.scatterplot(\n","    x=X_digits_tsne[:, 0], y=X_digits_tsne[:, 1], hue=y_digits, palette=\"tab10\", ax=ax[1], s=50, alpha=0.7\n",")\n","ax[1].set_title(f\"t-SNE Visualization of Digits Dataset\\n(Time: {time_tsne_digits:.2f} seconds)\", fontsize=16)\n","ax[1].set_xlabel(\"t-SNE Component 1\")\n","ax[1].set_ylabel(\"t-SNE Component 2\")\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaZwH1obHQEe"},"outputs":[],"source":["Из этих примеров видно, что t-SNE более продуктивно производит снижение размерности данных, но требует больше времени на реализацию:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwaYWYS6q-yf"},"outputs":[],"source":["# Печать времени выполнения\n","print(\"Время выполнения для набора данных Iris:\")\n","print(f\"PCA: {time_pca_iris:.4f} секунд\")\n","print(f\"t-SNE: {time_tsne_iris:.4f} секунд\")\n","\n","print(\"\\nВремя выполнения для набора данных Digits:\")\n","print(f\"PCA: {time_pca_digits:.4f} секунд\")\n","print(f\"t-SNE: {time_tsne_digits:.4f} секунд\")"]},{"cell_type":"markdown","metadata":{"id":"2s4TNTxOq92-"},"source":["t-SNE можно использовать для визуализации многомерных данных, но для обучения моделей он не подходит: мы не можем применить ранее обученную модель t-SNE к новым данным, поскольку для вычисления t-SNE требуется весь набор данных для оценки нелинейных зависимостей.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}