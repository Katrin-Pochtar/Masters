{"cells":[{"cell_type":"markdown","metadata":{"id":"A79WfSKcSE1s"},"source":["# Содержание\n","- [K-means](#1)\n","- [DBSCAN](#2)\n","- [Агломеративная кластеризация](#3)"]},{"cell_type":"markdown","metadata":{"id":"P_koFnvIUJh8"},"source":["<a name='1'></a>\n","## K-Means"]},{"cell_type":"markdown","metadata":{"id":"JsDJLAo9UJh8"},"source":["K-Means (k-средних) — это алгоритм кластеризации, который используется для разделения набора данных на группы (кластеры) на основе их схожести. Алгоритм старается минимизировать суммарное квадратичное отклонение между точками внутри каждого кластера и центроидами (средними значениями) этих точек.\n","\n","Процесс работы алгоритма k-Means следующий:\n","\n","1. Инициализация: Выбирается количество кластеров k и случайно инициализируются k центроидов.\n","2. Присваивание точек к кластерам: Каждая точка данных присваивается к ближайшему центроиду на основе евклидова расстояния или другой метрики.\n","3. Пересчет центроидов: Вычисляются новые центроиды путем нахождения средних значений точек в каждом кластере.\n","4. Повторение шагов 2 и 3: Шаги 2 и 3 повторяются до сходимости алгоритма, то есть до тех пор, пока точки перестают изменять свою принадлежность к кластерам или достигнут предел максимального количества итераций.\n","\n","В результате работы алгоритма k-Means мы получаем k кластеров, в каждом из которых точки схожи между собой и отличаются от точек в других кластерах. Кластеризация данных с помощью k-Means может быть полезна для группировки и классификации данных, выявления скрытых закономерностей и понимания структуры данных."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF5btTYzwmDr"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gobgeu8UUJh9"},"outputs":[],"source":["def kmeans(X, K, max_iters=100):\n","    # Инициализация центроидов случайным образом\n","    centroids = X[np.random.choice(range(len(X)), K, replace=False)]\n","\n","    for _ in range(max_iters):\n","        # Нахождение ближайшего центроида для каждой точки\n","        labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centroids, axis=-1), axis=-1)\n","\n","        # Обновление центроидов\n","        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n","\n","        # Проверка условия сходимости\n","        if np.all(centroids == new_centroids):\n","            break\n","\n","        centroids = new_centroids\n","\n","    return centroids, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgB4yxEIXBLZ"},"outputs":[],"source":["# Пример использования\n","X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n","K = 2\n","\n","centroids, labels = kmeans(X, K)\n","print(\"Центроиды:\")\n","print(centroids)\n","print(\"Метки кластеров:\")\n","print(labels)"]},{"cell_type":"markdown","metadata":{"id":"muka9QEc7wXa"},"source":["### Недостатки алгоритма\n","\n","K-Means не справляется с кластерами сложной формы (например, кольцевыми или пересекающимися кластерами). Метод также зависит от начальной инициализации центроидов, что может привести к различным результатам.\n","Требуется заранее задавать количество кластеров k.\n","\n","### Методы выбора k\n","\n","- Метод локтя (Elbow Method): Построить график зависимости суммы внутрикластерных квадратов расстояний (`inertia`) от количества кластеров и выбрать k, где график «ломается». `inertia` — это метрика, которая показывает, насколько близко точки внутри одного кластера расположены к своему центроиду. Чем меньше значение, тем плотнее точки внутри кластера.\n","\n","- Силуэтный анализ (Silhouette Score): Оценить качество кластеризации на основе расстояния между точками и их кластерами."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7TFq4C18EEE"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","\n","# Генерация искусственных данных\n","X, y = make_blobs(n_samples=300, centers=5, cluster_std=1.0, random_state=42)\n","\n","# Список для хранения значения \"inertia\" (суммы внутрикластерных квадратов расстояний)\n","inertia = []\n","\n","# Диапазон количества кластеров для тестирования\n","k_values = range(1, 11)\n","\n","# Построение модели K-Means для каждого значения k\n","for k in k_values:\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(X)\n","    inertia.append(kmeans.inertia_)\n","\n","# Построение графика метода локтя\n","plt.figure(figsize=(8, 5))\n","plt.plot(k_values, inertia, marker='o', linestyle='--')\n","plt.xlabel('Количество кластеров (k)')\n","plt.ylabel('Inertia (внутрикластерная сумма квадратов)')\n","plt.title('Метод локтя для выбора оптимального k')\n","plt.xticks(k_values)\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Lg1FkSgw9G5J"},"source":["Оптимальное значение K соответствует точке, где `inertia` перестает значительно уменьшаться. В данном случае, имеет смысл зафиксировать K=4 (занятно, что данные при этом сгенерированы исходя из K=5)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPAx6KY69pT9"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","\n","# Генерация искусственных данных\n","X, y = make_blobs(n_samples=300, centers=5, cluster_std=1.0, random_state=42)\n","\n","# Значения k для визуализации\n","k_values = [2, 3, 4, 10]\n","\n","# Создание графиков\n","fig, axes = plt.subplots(1, len(k_values), figsize=(20, 5))\n","\n","for ax, k in zip(axes, k_values):\n","    # Инициализация и обучение K-Means\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    y_kmeans = kmeans.fit_predict(X)\n","\n","    # Визуализация кластеров\n","    ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7)\n","    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","               c='red', marker='x', s=100, label='Центроиды')\n","    ax.set_title(f'K-Means с K={k}')\n","    ax.set_xlabel('Признак 1')\n","    ax.set_ylabel('Признак 2')\n","    ax.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rrzOM9wQUJh-"},"source":["<a name='2'></a>\n","## DBSCAN\n","\n","DBSCAN (Density-Based Spatial Clustering of Applications with Noise) - это алгоритм кластеризации данных, основанный на плотности. Он идентифицирует кластеры, исходя из плотности точек в пространстве данных, а также обнаруживает выбросы, которые не принадлежат ни к одному кластеру.\n","\n","Процесс работы алгоритма DBSCAN следующий:\n","\n","1. Выбор начальной точки: Выбирается случайная точка, которая еще не была посещена и не является выбросом.\n","2. Поиск плотной области: Алгоритм расширяется от выбранной начальной точки, посещая ближайшие точки в заданном радиусе epsilon. Если в заданной окрестности (эпсилон-окрестности) находится минимальное количество точек, необходимое для формирования кластера, то точка считается ядром (core point).\n","3. Расширение кластера: Для каждого ядра образуется кластер, который включает все точки, достижимые из этого ядра в заданном радиусе epsilon. Для этого рекурсивно ищутся все плотные точки в окрестности ядра.\n","4. Поиск выбросов: Точки, которые не достижимы из ядерных точек, считаются выбросами (noise points). Они не принадлежат ни к одному кластеру.\n","5. Повторение шагов 2-4: Шаги 2-4 повторяются для всех непосещенных точек данных до тех пор, пока все точки не будут просмотрены.\n","\n","### Основные параметры\n","- ε (eps): Радиус окрестности точки.\n","- MinPts: Минимальное количество точек для формирования плотного кластера.\n","\n","### Преимущества DBSCAN\n","- Работает с кластерами произвольной формы.\n","- Способен выявлять шум (выбросы).\n","\n","\n","### Недостатки DBSCAN\n","- Чувствителен к параметрам ε и MinPts.\n","- Плохо масштабируется для больших данных."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xec2hd7rUJh_"},"outputs":[],"source":["class DBSCAN:\n","    def __init__(self, eps, min_samples):\n","        self.eps = eps\n","        self.min_samples = min_samples\n","        self.labels = None\n","\n","    def euclidean_distance(self, x1, x2):\n","        return np.sqrt(np.sum((x1 - x2) ** 2))\n","\n","    def fit(self, X):\n","        self.labels = np.zeros(len(X))  # Инициализируем массив меток кластера нулями для каждой точки в датасете\n","        cluster_label = 0  # Инициализируем счетчик меток кластера\n","\n","        for i in range(len(X)):  # Проходим по всем точкам в датасете\n","            if self.labels[i] != 0:  # Если точка уже имеет метку кластера, переходим к следующей точке\n","                continue\n","\n","            neighbors = self._find_neighbors(X, i)  # Находим соседей для текущей точки\n","\n","            if len(neighbors) < self.min_samples:  # Если количество соседей меньше минимального требования\n","                self.labels[i] = -1  # Присваиваем точке метку выброса (-1)\n","            else:\n","                cluster_label += 1  # Инкрементируем счетчик меток кластера\n","                self._expand_cluster(X, i, neighbors, cluster_label)  # Расширяем кластер, начиная с текущей точки\n","\n","\n","    def _expand_cluster(self, X, point_index, neighbors, cluster_label):\n","        \"\"\"\n","        В этом методе сначала присваивается выбранной точке `point_index` метка кластера `cluster_label`.\n","\n","        Затем идет цикл `while`, который выполняется, пока не исчерпаны все соседи в списке `neighbors`.\n","\n","        На каждой итерации цикла берется очередной сосед из списка `neighbors` и проверяется его метка кластера:\n","\n","        - Если метка равна `-1`, это означает, что сосед не имеет метки кластера.\n","            В этом случае ему присваивается метка кластера `cluster_label`.\n","        - Если метка равна `0`, это означает, что сосед не принадлежит ни одному кластеру.\n","            В этом случае ему также присваивается метка кластера `cluster_label`.\n","        Затем для этого соседа находятся новые соседи с помощью метода `_find_neighbors`.\n","            - Если количество новых соседей больше или равно минимальному требованию `min_samples`,\n","                то эти новые соседи добавляются в список `neighbors`.\n","\n","        В конце каждой итерации цикла инкрементируется счетчик `i` для перехода к следующему\n","        соседу в списке `neighbors`.\n","\n","        \"\"\"\n","        self.labels[point_index] = cluster_label  # Присваиваем метку кластера выбранной точке\n","\n","        i = 0\n","        while i < len(neighbors):  # Пока не исчерпаны все соседи\n","            neighbor = neighbors[i]  # Берем очередного соседа\n","\n","            if self.labels[neighbor] == -1:  # Если сосед не имеет метки кластера\n","                self.labels[neighbor] = cluster_label  # Присваиваем ему метку кластера\n","            elif self.labels[neighbor] == 0:  # Если сосед не принадлежит ни одному кластеру\n","                self.labels[neighbor] = cluster_label  # Присваиваем ему метку кластера\n","                new_neighbors = self._find_neighbors(X, neighbor)  # Находим новых соседей для данной точки\n","\n","                if len(new_neighbors) >= self.min_samples:  # Если количество новых соседей больше или равно минимальному требованию\n","                    neighbors += new_neighbors  # Добавляем новых соседей в список соседей\n","            i += 1  # Переходим к следующему соседу\n","\n","\n","    def _find_neighbors(self, X, point_index):\n","        # Инициализируем пустой список для хранения индексов соседних точек\n","        neighbors = []\n","        for i in range(len(X)):  # Проходим по всем точкам в датасете\n","            # Если расстояние между текущей точкой и точкой с индексом i меньше или равно eps\n","            if self.euclidean_distance(X[point_index], X[i]) <= self.eps:\n","                neighbors.append(i)  # Добавляем индекс соседней точки в список neighbors\n","        return neighbors  # Возвращаем список соседних точек\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H30nPJPPUJiA"},"outputs":[],"source":["# Пример использования\n","X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n","eps = 2\n","min_samples = 2\n","\n","dbscan = DBSCAN(eps, min_samples)\n","dbscan.fit(X)\n","\n","print(\"Метки кластеров:\")\n","print(dbscan.labels)"]},{"cell_type":"markdown","metadata":{"id":"0sCcK7-i_0VM"},"source":["Благодаря способности выявлять выбросы в данных, алгоритм DBSCAN удобно использовать для поиска аномалий в данных."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjaRsdxPUDVJ"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import DBSCAN\n","import matplotlib.pyplot as plt\n","\n","# Генерация данных с несколькими кластерами\n","X, _ = make_blobs(n_samples=1500, centers=4, cluster_std=1.6, random_state=42)\n","\n","# Масштабируем данные\n","X_scaled = StandardScaler().fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"pQTi6qrBUF6g"},"source":["Определим оптимальное значение `eps` с помощью NearestNeighbors и локтевой диаграммы."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dv3C3cXYB2Qf"},"outputs":[],"source":["from sklearn.neighbors import NearestNeighbors\n","import numpy as np\n","\n","# Найдем оптимальное значение eps с помощью графика k-distance\n","nearest_neighbors = NearestNeighbors(n_neighbors=5)\n","nearest_neighbors.fit(X_scaled)\n","distances, indices = nearest_neighbors.kneighbors(X_scaled)\n","\n","# Сортируем расстояния для визуализации\n","distances = np.sort(distances[:, 4])  # Берем расстояния до 5-го соседа\n","plt.figure(figsize=(8, 6))\n","plt.plot(distances)\n","plt.title('График расстояний (k-distance graph)')\n","plt.xlabel('Точки данных (индексы)')\n","plt.ylabel('Расстояние до 5-го ближайшего соседа')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpqsXdIcEXUh"},"outputs":[],"source":["# Применяем DBSCAN с подобранным eps\n","dbscan = DBSCAN(eps=0.17, min_samples=25)  # Используйте значение eps, которое подходит для ваших данных\n","y_dbscan = dbscan.fit_predict(X_scaled)\n","\n","# Визуализация результатов\n","plt.figure(figsize=(10, 7))\n","unique_labels = set(y_dbscan)\n","\n","for label in unique_labels:\n","    if label == -1:  # Шумовые точки\n","        color = 'red'\n","        label_name = 'Шум'\n","    else:\n","        color = plt.cm.viridis(label / max(unique_labels))\n","        label_name = f'Кластер {label}'\n","\n","    plt.scatter(X_scaled[y_dbscan == label, 0], X_scaled[y_dbscan == label, 1],\n","                c=color, label=label_name, s=10, alpha=0.7)\n","\n","plt.title('DBSCAN на сгенерированных данных (4 кластера)')\n","plt.xlabel('Признак 1 (норм.)')\n","plt.ylabel('Признак 2 (норм.)')\n","plt.legend(markerscale=2)\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"NWUG31_RUJiA"},"source":["<a name='3'></a>\n","## Агломеративная кластеризация\n","\n","Агломеративная кластеризация (агломеративный метод) - это алгоритм иерархической кластеризации, который последовательно объединяет близкие точки данных в кластеры, формируя дерево иерархии кластеров, называемое дендрограммой.\n","\n","Процесс работы агломеративной кластеризации следующий:\n","\n","1. Инициализация: Каждая точка данных начинает в качестве отдельного кластера.\n","2. Вычисление матрицы расстояний: Вычисляется матрица расстояний между каждой парой кластеров. Расстояние может быть определено различными способами, такими как евклидово расстояние или корреляция.\n","3. Объединение ближайших кластеров: Два ближайших кластера (т.е. с наименьшим расстоянием между ними) объединяются в один новый кластер.\n","4. Обновление матрицы расстояний: Матрица расстояний обновляется, чтобы отразить расстояния между новым объединенным кластером и остальными кластерами.\n","5. Повторение шагов 3 и 4: Шаги 3 и 4 повторяются до тех пор, пока все точки данных не объединятся в один кластер или достигнется заранее заданное количество кластеров.\n","6. Построение дендрограммы: На основе последовательности объединений строится дендрограмма, которая визуализирует иерархию кластеров.\n","\n","Результат агломеративной кластеризации представляет собой дерево иерархии кластеров, где каждый узел представляет собой кластер, а расстояние между узлами отображает степень их схожести. По анализу дендрограммы можно определить оптимальное количество кластеров, а также их структуру и иерархические отношения. Агломеративная кластеризация особенно полезна в задачах, где требуется иерархическое представление кластеров или когда количество кластеров неизвестно заранее."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPiMq73CUJiA"},"outputs":[],"source":["class AgglomerativeClustering:\n","    def __init__(self, n_clusters):\n","        self.n_clusters = n_clusters\n","        self.labels = None\n","\n","    def fit(self, X):\n","        # Инициализация каждой точки как отдельного кластера\n","        clusters = [[x] for x in X]\n","\n","        while len(clusters) > self.n_clusters:  # Пока количество кластеров больше заданного числа\n","            min_dist = np.inf  # Инициализация минимального расстояния как бесконечность\n","            merge_indices = (0, 0)  # Инициализация индексов кластеров для объединения\n","\n","            # Находим два ближайших кластера для объединения\n","            for i in range(len(clusters)):  # Проходим по каждому кластеру\n","                for j in range(i+1, len(clusters)):  # Проходим по остальным кластерам\n","                    dist = self._distance(clusters[i], clusters[j])  # Вычисляем расстояние между кластерами\n","\n","                    if dist < min_dist:  # Если текущее расстояние меньше минимального\n","                        min_dist = dist  # Обновляем значение минимального расстояния\n","                        merge_indices = (i, j)  # Обновляем индексы кластеров для объединения\n","\n","            # Объединяем два ближайших кластера\n","            merged_cluster = clusters[merge_indices[0]] + clusters[merge_indices[1]]\n","\n","            # Удаляем объединенные кластеры из списка\n","            del clusters[merge_indices[1]]\n","            del clusters[merge_indices[0]]\n","\n","            # Добавляем объединенный кластер в список\n","            clusters.append(merged_cluster)\n","\n","        # Присваиваем метки кластеров точкам\n","        self.labels = self._assign_labels(clusters, X)\n","\n","\n","    def _distance(self, cluster1, cluster2):\n","        min_dist = np.inf  # Инициализируем переменную минимального расстояния как бесконечность\n","\n","        for point1 in cluster1:  # Проходим по каждой точке в первом кластере\n","            for point2 in cluster2:  # Проходим по каждой точке во втором кластере\n","                dist = np.linalg.norm(point1 - point2)  # Вычисляем Евклидово расстояние между двумя точками\n","                if dist < min_dist:  # Если текущее расстояние меньше минимального\n","                    min_dist = dist  # Обновляем значение минимального расстояния\n","\n","        return min_dist  # Возвращаем минимальное расстояние\n","\n","\n","    def _assign_labels(self, clusters, dataset):\n","        labels = np.zeros(dataset.shape[0], dtype=int)  # Заполняем для каждой точки массив меток нулями\n","\n","        for index, point in enumerate(dataset):  # Проходим по каждой  точке в датасете\n","            for cluster_index, cluster in enumerate(clusters): # Проходим по всем кластерам\n","                if np.sum(np.all(cluster == point, axis=1)): # Проверяем вхождение точки в текущий кластер\n","                    labels[index] = cluster_index  # Присваиваем метку кластера текущей точке\n","                    continue\n","\n","        return labels  # Возвращаем массив меток\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_8f8x69UJiB"},"outputs":[],"source":["# Пример использования\n","X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n","n_clusters = 2\n","\n","agglomerative = AgglomerativeClustering(n_clusters)\n","agglomerative.fit(X)\n","\n","print(\"Метки кластеров:\")\n","print(agglomerative.labels)"]},{"cell_type":"markdown","metadata":{"id":"8H7-jyhxRd-i"},"source":["### Методы объединения кластеров\n","\n","- Single linkage (ссылочное соединение) — расстояние между ближайшими точками двух кластеров.\n","- Complete linkage (полное соединение) — расстояние между самыми удалёнными точками двух кластеров.\n","- Average linkage (среднее соединение) — среднее расстояние между всеми точками двух кластеров.\n","- Ward linkage — минимизация суммы квадратов отклонений внутри кластеров, что приводит к более компактным кластерам.\n","\n","В примере ниже мы используем метод `single`, который приводит к \"цепочечному\" объединению кластеров. Попробуйте различные методы, чтобы увидеть их влияние на структуру кластеров.\n","\n","### Размерность данных\n","При использовании агломеративной кластеризации важно учитывать размерность данных. На высокоразмерных данных дендрограмма может быть трудной для интерпретации, и рекомендуется использовать методы снижения размерности, такие как PCA (главные компоненты), для визуализации.\n","\n","### Точки на границе кластеров\n","Агломеративная кластеризация не всегда может дать чёткие результаты на данных, где кластеры сильно перекрываются. Это важно учитывать, когда вы выбираете метрики расстояния и методы агломерации.\n","\n","### Число кластеров `n_clusters`\n","В агломеративной кластеризации количество кластеров определяется заранее. В случае, если вы хотите визуализировать результат в виде дендрограммы, оптимальное количество кластеров можно определить, проведя анализ на дендрограмме, например, с использованием \"обрезки\" дерева на определённом уровне."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOJUP7i2RRob"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.datasets import make_blobs\n","\n","# Генерация случайных данных для кластеризации\n","X, _ = make_blobs(n_samples=20, centers=3, random_state=42)\n","\n","# Применение агломеративной кластеризации с использованием scipy\n","linked = linkage(X, 'single')  # Использование метода 'single' для вычисления расстояний\n","\n","# Построение дендрограммы\n","plt.figure(figsize=(8, 6))\n","dendrogram(linked)\n","plt.title(\"Дендрограмма для агломеративной кластеризации\")\n","plt.xlabel(\"Индекс объекта\")\n","plt.ylabel(\"Расстояние\")\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yv2JinLnS5FZ"},"source":["Для примера работы агрометративной кластеризации, давайте рассмотрим как она справляется с кластеризацией на уже знакомом нам наборе данных Iris Petals. Для удобства визуализации, мы возьмём только два признака."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BA9TLFXaR4H1"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.decomposition import PCA\n","\n","# Загрузка набора данных Ирисы Фишера\n","iris = load_iris()\n","X = iris.data[:, 1:]  # Два признака данных\n","y = iris.target  # Истинные метки классов (для сравнения)\n","\n","# Применение агломеративной кластеризации\n","n_clusters = 3  # У нас три типа ирисов, поэтому количество кластеров = 3\n","agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n","labels = agg_clustering.fit_predict(X)\n","\n","\n","# Визуализация результатов кластеризации\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n","plt.title(\"Агломеративная кластеризация на данных о цветках ириса\")\n","plt.xlabel(\"PC1 (Главная компонента 1)\")\n","plt.ylabel(\"PC2 (Главная компонента 2)\")\n","plt.colorbar(label='Кластер')\n","plt.show()\n","\n","# Для сравнения с истинными метками:\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50)\n","plt.title(\"Истинные метки классов на данных о цветках ириса\")\n","plt.xlabel(\"PC1 (Главная компонента 1)\")\n","plt.ylabel(\"PC2 (Главная компонента 2)\")\n","plt.colorbar(label='Метка класса')\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}